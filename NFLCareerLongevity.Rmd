---
title: "Can You Predict When NFL Players Will Retire?"
author: "Imogen, Adam and Nolan"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
library(dplyr)
library(tidyverse)
library(caret)
library(lme4)
library(tidyverse)
library(reticulate)
knitr::opts_chunk$set(echo = TRUE)

```

## Abstract
In this research project, we will attempt to forecast NFL players' retirement dates, mainly on the basis of their positions. We will use logistic regression to predict the probability of retirement within the next three years and a random effects model to see the effect of different positions on years remaining in the league. These models will take into account variables like age, games played, number of concussions, and number of total injuries. We will look into how these variables affect the probability of a player retiring within the next 3 years or not, as well as how they affect positions differently regarding the number of years they have left in the NFL. This report will also record the difficulties we faced with data cleaning and merging. Due to the lack of full data, we will be sampling all data from years 2010-2024 so better account for modern injury technology Our goal is to assess the factors that relate to “premature retirements.” We hypothesize that the various positions are what affect NFL careers' length. This, in turn, could help teams and analysts predict and plan for a better player turnover.

## Introduction
It’s no secret that, while the NFL is an exciting league, there is a struggle with career longevity. Star players such as Andrew Luck and Calvin Johnson retired before they even turned 30-years-old. In addition to this, several star players missed key playing time in what would be their prime years due to injuries. Teams and analysts need software that can predict when one of their players might retire (either medically or by choice) based on their current workload and performance. We will create this by first attempting to understand the factors that influence NFL players' career longevity. Position-specific demands, injury risks, and player workload all play a significant role in determining the length of a player’s career. We will analyze historical data in efforts to identify patterns that may help predict the retirement year of a player. In this project, we will use data from 2010 to 2024 to develop a predictive model for retirement timelines across different NFL positions. 

## Proposed Solution and Methodology
We will gather seasonal data at a player level of detail from Kaggle (in game stats) and the Injury Report section of the NFL website. We will then:
1. Clean our data by normalizing names of positions and teams, and removing years that have inconsistent data.
2. Aggregate our two data sources via the creation of a player Id.
3. Make the player-season data cumulative so that each row is a different data point, instead of having to worry about stratified sampling or keeping track of individual time-series for every player's career when doing our train-test split.
4. Explore our variables and summary statistics to see the quality of our predictors.
5. Run our logistic and linear models by position and evaluate/tune for accuracy.


## Data Acquisition
### Webscraping for NFL Injury Reports

```{python Injury Report Scrape, eval = FALSE}
import bs4
import requests
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import time
import pandas as pd
import itertools
from ast import literal_eval
import ast
import math
import re
import matplotlib.pyplot as plt
from difflib import SequenceMatcher
from collections import Counter
import collections
import copy
import random

import warnings
warnings.filterwarnings('ignore')

pd.set_option('display.max_columns', None)
pd.set_option('display.float_format', '{:.2f}'.format)
urls_list = []

def create_list_urls(urls, start):
    
    for y in range(start,start+1):
        #create regular season urls
        for w in range(1,18):
            urls.append('https://www.nfl.com/injuries/league/' + str(y) + '/reg' + str(w))
        
        #create post season urls
        for i in range(1,5):
            urls.append('https://www.nfl.com/injuries/league/' + str(y) + '/post' + str(i))

        #create pro season urls
        urls.append('https://www.nfl.com/injuries/league/' + str(y) + '/pro1')    
            
def create_dfs():
    counts = players_df.groupby(['Player', 'Season', 'Position', 'Team']).size().reset_index(name='TotalInjuryCount')
    sorted_df = counts.sort_values(by='TotalInjuryCount', ascending=False)
    sorted_df.to_csv('totalInjury.csv',mode='a', index=False)
    injury_counts = players_df.groupby(['Player', 'Season', 'Position', 'Team', 'Injury']).size().unstack(fill_value=0).reset_index()



    filtered_df = injury_counts.filter(like='Concussion' )
    concussion_sum = filtered_df.sum(axis=1)
    # Add the sum as a new column to the original DataFrame
    filtered_df['ConcussionTotal'] = concussion_sum
    merged_df = pd.concat([counts, filtered_df['ConcussionTotal']], axis=1)
    concussion_sort = merged_df.sort_values(by='ConcussionTotal', ascending=False)
    concussion_sort.to_csv('totalConcussion.csv',mode='a', index=False)

    practice_counts = players_df.groupby(['Player', 'Season', 'Position', 'Team', 'Practice']).size().unstack(fill_value=0).reset_index()
    practice_counts.drop(practice_counts.columns[4], axis=1, inplace=True)
    practice_counts.to_csv('practice.csv',mode='a', index=False)

    status_counts = players_df.groupby(['Player', 'Season', 'Position','Team', 'GameStatus']).size().unstack(fill_value=0).reset_index()
    status_counts.drop(status_counts.columns[4], axis=1, inplace=True)
    status_counts.to_csv('status.csv',mode='a', index=False)
    
def injury_data(url_list):
    all_players = []
    df_list = []
    df = pd.DataFrame()
    
    for i in url_list:
        print('Injuries from ' + i)
        curr_season = re.search("\d\d\d\d", i).group()
        try:
            alpha = requests.get(i)
            beta = bs4.BeautifulSoup(alpha.text)
            tables = beta.findAll("table")
            teams = beta.findAll("div", class_="d3-o-section-sub-title")
            idx = 0

            for table in tables:
                if table.findParent("table") is None:
                    tbody = table.find("tbody")
                    for tr in tbody:
                        try:
                            td = tr.find_all("td") 
                            rows = [tr.text.strip() for tr in td if tr is not None and len(tr) > 0]
                            for i in rows:
                                df_list.append(i) #at the player level
                            df_list.append(curr_season)
                            df_list.append(teams[idx].find("span").text)
                            all_players.append(copy.copy(df_list))
                            df_list = []
                    
                        except:
                            pass
                idx += 1        
                        
        except:
            print("Error with url: " + i)
        print(len(all_players))
    return all_players
        
        
for yr in range(2008, 2024):  
    urls_list = []  
    create_list_urls(urls_list, yr)
    players_df = pd.DataFrame(injury_data(urls_list), columns = ['Player', 'Position', 'Injury', 'Practice', 'GameStatus', 'Season', 'Team'])
    create_dfs()
    r = random.uniform(1, 5)

    print("Written to df. Waiting " + str(r))
    time.sleep(60 * r)
    saved = players_df
    
    
    
    
def create_dfs():
    counts = players_df.groupby(['Player', 'Season', 'Position', 'Team']).size().reset_index(name='TotalInjuryCount')
    sorted_df = counts.sort_values(by='TotalInjuryCount', ascending=False)
    sorted_df.to_csv('totalInjury.csv', index=False)
    injury_counts = players_df.groupby(['Player', 'Season', 'Position', 'Team', 'Injury']).size().unstack(fill_value=0).reset_index()



    filtered_df = injury_counts.filter(like='Concussion' )
    concussion_sum = filtered_df.sum(axis=1)
    # Add the sum as a new column to the original DataFrame
    filtered_df['ConcussionTotal'] = concussion_sum
    merged_df = pd.concat([counts, filtered_df['ConcussionTotal']], axis=1)
    concussion_sort = merged_df.sort_values(by='ConcussionTotal', ascending=False)
    concussion_sort.to_csv('totalConcussion.csv', index=False)

    practice_counts = players_df.groupby(['Player', 'Season', 'Position', 'Team', 'Practice']).size().unstack(fill_value=0).reset_index()
    practice_counts.drop(practice_counts.columns[4], axis=1, inplace=True)
    practice_counts.to_csv('practice.csv', index=False)

    status_counts = players_df.groupby(['Player', 'Season', 'Position','Team', 'GameStatus']).size().unstack(fill_value=0).reset_index()
    status_counts.drop(status_counts.columns[4], axis=1, inplace=True)
    status_counts.to_csv('status.csv', index=False)

```

### Kaggle Dataset

```{r Read in Data, warning = FALSE}
all_seasons <- read.csv("NFL Player Stats(1922 - 2022).csv")
head(all_seasons)
seasons <- all_seasons[all_seasons$Season >= 2006,] #only have injury data from 2006 onwards
seasons <- seasons %>%
  select(Tm, Season, Player, Age, Pos, G, GS, AllTD, Pts, FGM, FGA)
head(seasons)

all_injuries <- read.csv("totalConcussion.csv") #injury and concussion by player and year
all_injuries <-  all_injuries %>% separate(Player, into = c("FirstName", "LastName"), sep = " ")
all_injuries <- all_injuries[all_injuries$Position != 'Position',]

#clean seasons
seasons <-  seasons %>% separate(Player, into = c("FirstName", "LastName"), sep = " ")
seasons$LastName <- gsub("\\+$", "", seasons$LastName)
seasons <-  seasons %>% mutate(AllTD = ifelse(is.na(AllTD),0, AllTD))



```
## Data Cleaning
### Positions

```{r Clean Position Col, warning = FALSE}
seasons <- seasons %>%
  mutate(Pos = ifelse(Pos %in% c("LCB", "CB", "RCB", "RCB/LCB", "DB/RCB", "RCB/LCB/DB", "LCB/RCB", "DB/LCB", "RCB/DB",
                                 "SS", "FS", "SS/FS", "LCB/FS", "DB/FS", "FS/SS", "SS/RLB", "S", "DB"), "DB", Pos))

seasons <- seasons %>%
  mutate(Pos = ifelse(Pos %in% c("MLB", "ROLB", "LLB/ROLB", "RLB", "RLB/MLB", "LOLB", "LLB", "LILB", "LB", "LILB/ROL", 
                                 "RILB", "LLB/RLB", "LB/LLB", "LLB/MLB", "LILB/RIL", "ROLB/LOL", "OLB", "MLB/RLB"), "LB", Pos))

seasons <- seasons %>%
  mutate(Pos = ifelse(Pos %in% c("LT/RT", "RT/LT", "LT", "T", "NT", "RT"), "T", Pos))

seasons <- seasons %>%
  mutate(Pos = ifelse(Pos %in% c("LDT/RDT", "LDT", "DT", "DT/LDT", "RDT", "DL"), "DT", Pos))

seasons <- seasons %>%
  mutate(Pos = ifelse(Pos %in% c("RDE/LDE", "RDE", "DE", "LDE/RDE", "LDE"), "DE", Pos))

seasons <- seasons %>%
  mutate(Pos = ifelse(Pos %in% c("LG", "G"), "G", Pos))

seasons <- seasons %>%
  mutate(Pos = ifelse(Pos %in% c("C/LG"), "C", Pos))

seasons <- seasons %>%
  mutate(Pos = ifelse(Pos %in% c("RB/TE", "FB", "DT/FB"), "FB", Pos))

seasons <- seasons %>%
  mutate(Pos = ifelse(Pos %in% c("QB", "WR/QB"), "QB", Pos))

all_injuries <- all_injuries %>%
  mutate(Position = ifelse(Position %in% c("CB", "DB", "S"), "DB", Position))

seasons <- seasons[seasons$Pos != "",]
all_injuries <- all_injuries[all_injuries$Position != "LS",]
all_injuries <- all_injuries[all_injuries$Position != "KR",]
all_injuries <- all_injuries[all_injuries$Position != "PR",]

#theyre now equal
sort(unique(all_injuries$Position))
sort(unique(seasons$Pos))


```
### Teams
```{r Clean Team Col, warning = FALSE}

seasons$Team <- seasons$Tm

InjuriesSeason <- inner_join(all_injuries, seasons, by = "Team")

patterns <- c("Seahawks","Steelers", "Cowboys", "Lions", "Browns",       
"Chiefs","Dolphins","Eagles", "Buccaneers", "Vikings", "Saints", "Packers",
"Colts", "Falcons", "Bengals","Raiders", "Broncos", "Ravens", "Commanders",
"Giants", "49ers", "Jets", "Jaguars", "Panthers", "Bears", "Texans", "Patriots",
"Rams", "Chargers", "Titans", "Bills", "Cardinals", "Football Team", "Redskins",
"Niners")
replacements <- c("SEA", "PIT", "DAL", "DET", "CLE", "KAN", "MIA", "PHI",
              "TAM", "MIN", "NOR", "GNB", "IND", "ATL", "CIN", "LVR",
              "DEN", "BAL", "WAS", "NYG", "SFO", "NYJ", "JAX", "CAR", "CHI",
              "HOU", "NWE", "LAR", "LAC", "TEN", "BUF", "ARI", "WAS", "WAS", "SFO")
for (i in 1:length(patterns)) {
  all_injuries$Team <- gsub(patterns[i], replacements[i], all_injuries$Team)
}

#theyre now equal
head(all_injuries)
head(seasons)
```

### Merge on PlayerID

```{r Make the PlayerIDs, warning = FALSE}
#make injury ID to merge
all_injuries$PlayerID <- paste0(paste0(paste0(substring(all_injuries$FirstName, 1, 1), all_injuries$LastName), all_injuries$Position), all_injuries$Season)

#make season ID to merge
seasons$PlayerID <- paste0(paste0(paste0(substring(seasons$FirstName, 1, 1), seasons$LastName), seasons$Pos), seasons$Season)

```


```{r warning = FALSE}
#MERGE ON INJURIES AND SEASONS
injuries <-  all_injuries %>% select(PlayerID, TotalInjuryCount, ConcussionTotal)
seasons2010 <- seasons[seasons$Season >= 2010,] %>% select(-Tm)
joined <- left_join(seasons2010, injuries, by = 'PlayerID')
joined <- joined %>% mutate(ConcussionTotal = ifelse(is.na(ConcussionTotal),0, ConcussionTotal))
joined <- joined %>% mutate(TotalInjuryCount = ifelse(is.na(TotalInjuryCount),0, TotalInjuryCount))

head(joined)
```


```{r Make final dataset, warning = FALSE}
#Making cumulative cols and response var, this way we wouldn't have to use stratified sampling to keep players together
final <- joined
final <- final %>%
  group_by(FirstName, LastName, Pos) %>%
  arrange(Season) %>%
  mutate(G = cumsum(G), GS = cumsum(GS), AllTD = cumsum(AllTD), Pts = cumsum(Pts),
         FGM = cumsum(FGM), FGA = cumsum((FGA)), ConcussionTotal = cumsum(ConcussionTotal), TotalInjuryCount = cumsum(TotalInjuryCount))


#create response var
final <- final %>% group_by(FirstName, LastName, Pos) %>%
  arrange(Season)%>% mutate(YearsRemaining = max(Season) - Season)

final_all <-  final 

#All positions
#filter positions that have very few samples
too_few <- final_all %>% group_by(Pos) %>% count(Pos)%>% filter(n < 100) %>% select(Pos) %>% ungroup() %>% pull(Pos)

#remove low sample size pos
final_all <- final_all %>% filter(!(Pos %in% too_few))

#table of average career lengths
average_len_careers <- final_all %>% group_by(FirstName, LastName, Pos) %>%
  arrange(Season)%>% mutate(careerLen = max(Season) - min(Season)) %>% ungroup() %>% select(Pos, careerLen) %>% group_by(Pos) %>% mutate(avgLen = mean(careerLen)) %>% distinct(Pos, avgLen) %>% arrange(desc(avgLen)) 

average_len_careers$avgLen <- round(average_len_careers$avgLen, 2)



#drop players that moved around a lot
final[final$Team != '3TM',]
final[final$Team != '2TM',]

#separate out kickers because they have extra cols for field goals
kickers_final <- final[final$Pos == 'K',]
final_1 <- final
final_1 <- final[final$Pos != 'K',] %>% select(-FGM, -FGA)

#Create logistic outcome variable
#1 = greater than and equal 3 years remaining
#0 = less than 3 years remaining
final$ThreeYrLeft <- ifelse(final$YearsRemaining >= 3, 1 ,0)

#add logistic outcome to kicker-free dataset
final_1$ThreeYrLeft <- ifelse(final_1$YearsRemaining >= 3, 1 ,0)
final_1 <- final_1 %>% select(-YearsRemaining)

#glimpse of data set
final_1[final_1$LastName == 'Brady',]
```
ThreeYearsLeft is 1, if you have 3 or more years left in the NFL and 0 if you have less than 3 years


## Variable Exploration
```{r Variable Exploration, warning = FALSE}
final %>% ungroup() %>%
  dplyr::select(YearsRemaining ,Age, G, GS, AllTD, Pts, TotalInjuryCount, ConcussionTotal) %>%
  pivot_longer(cols = -YearsRemaining) %>% 
  # Plot every value against whiffs
  ggplot(aes(value, YearsRemaining)) +
  geom_point() +
  
  geom_smooth(method = "lm") +
  # Put each variable into its own plot:
  facet_wrap(vars(name), scales = "free") + theme_minimal()

```
Age looks like it will be the best factor to predict years remaining as the trend line shows a marginal negative correlation, meaning as age increases the number of years remaining you have decreases.
Pts looks like the second most effective variable, as points increases, years remaining in the NFL increases. Perhaps this is because more successful players last longer.

All other variables look like they will be pretty negligible predictors.

```{r warning = FALSE}
graphing_data <- final_1 %>% ungroup() %>%
  dplyr::select(ThreeYrLeft, Age) 
graphing_data$ThreeYrLeft = as.factor(graphing_data$ThreeYrLeft)
graphing_data %>%
  pivot_longer(cols = -ThreeYrLeft) %>% 
  ggplot(aes(x = value, fill =ThreeYrLeft)) +
  geom_density() +
  theme_minimal() +
  scale_fill_manual(values = c("0" = "red", "1" = "blue"), 
                    labels = c("< 3 Years Left", ">= 3 Years Left")) +
  labs(title = "Density Plot of Age Variable")
```
As Age looked to be the most descriptive variable from our scatterplots, we chose to plot Age as a density plot to see if there would be a separation between the Age of players with greater than/less than 3 years remaining. From the plot, you can see there is not an exact separation, however the density of ages 27 years and older is greater for those that have less than 3 years left, which supports the idea that older players will have less time to retirement.

##Logistic Model for RBs

```{r RB Logistic, warning = FALSE }
set.seed(123)

finalRB <- final_1[final_1$Pos == 'RB',]

train_index <- createDataPartition(finalRB$ThreeYrLeft, p = 0.8, list = FALSE)
train_dataRB <- finalRB[train_index, ]
test_dataRB <- finalRB[-train_index, ]

# Check the dimensions
dim(train_dataRB)
dim(test_dataRB)


# Ensure as.factor(train_data$FiveYrLeft)
train_dataRB$ThreeYrLeft <- as.numeric(train_dataRB$ThreeYrLeft)
test_dataRB$ThreeYrLeft <- as.numeric(test_dataRB$ThreeYrLeft)


train_dataRB<- train_dataRB %>% ungroup() %>% select(ThreeYrLeft,Age, G, GS, AllTD, Pts, TotalInjuryCount, ConcussionTotal)

test_dataRB<- test_dataRB %>% ungroup() %>% select(ThreeYrLeft, Age, G, GS, AllTD, Pts, TotalInjuryCount, ConcussionTotal)

logisticRB <- glm(ThreeYrLeft ~ ., train_dataRB, family = "binomial")
summary(logisticRB)

sjPlot::plot_model(logisticRB, type ="pred") #part of logistic regression curve

#AIC: 1329.6
```
Our first model is a logistic model for RB using all features and of those features Age, Pts Scored, TD Scored and Games Played were significant. 

Age, Games and TD had negative coefficient so that as they increase, the odds of being in the league for 3 or more years decrease. Pts had a positive coefficient so as it increases, the odds of being in the league for 3 or more years increases.

```{r Logistic Test, warning = FALSE}
predictions <- predict(logisticRB, newdata = test_dataRB, type = "response")
test_pred <- ifelse(is.na(predictions), NA, ifelse(predictions > 0.5, 1, 0))
conf_matrix <- table(test_pred, test_dataRB$ThreeYrLeft)
conf_matrix
# Calculate metrics
precision <- conf_matrix[1, 1] / sum(conf_matrix[1, ])
recall <- conf_matrix[1, 1] / sum(conf_matrix[, 1])
f1_score <- 2 * (precision * recall) / (precision + recall)
accuracy <- mean(test_pred == test_dataRB$ThreeYrLeft)
precision
recall
f1_score
accuracy

```
Our basic logistic model has an overall accuracy of 69%. We will use 0 (less than 3 years) as positive for the following statistics as we are focusing on retiring players.

It has a precision of 0.7, meaning of all the players the model predicted to have < 3 years left, 70% actually did. So, out of the model’s positive predictions, 70% were correct.

It has a recall of 0.97, meaning of all the players that actually had < 3 years left, the model identified 97% of them correctly. It means the model found 97% of the actual positives in the data set.

It has an F1 score of 0.81, which is a high balanced metric of recall and precision indicating this is a good model for identifying positives accurately, when a player will retire in less than 3 years.


This model had an AIC of 1330. Next we ran a mixed effects linear model using the significant factors.

## Logistic Model with Interaction for RBs
```{r RB Mixed Effects, warning=FALSE}

logisticRB1 <- glm(ThreeYrLeft ~ Age *  G  * AllTD, train_dataRB, family = "binomial")
summary(logisticRB1)
#AIC: 1286

sjPlot::plot_model(logisticRB1, type ="pred")

predictions <- predict(logisticRB1, newdata = test_dataRB, type = "response")


```
At the 5% significance level, Age and the interaction between Games and Touchdowns had a negative correlation and the interaction between Age and Touchdowns had a positive correlation.

Performed threshold testing to see if there is a better threshold than 0.5 for our logistic regression.
```{r warning = FALSE}
#Threshold Testing
curr = 0
for (i in seq(0.3, 0.7, by = 0.01)){
  test_pred <- ifelse(is.na(predictions), NA, ifelse(predictions > i, 1, 0))
  
  accuracy <- mean(test_pred == test_dataRB$ThreeYrLeft)
  
  
  if (accuracy > curr){
    best = c(i, accuracy)
    curr = accuracy
  }
 
}

test_pred <- ifelse(is.na(predictions), NA, ifelse(predictions > best[1], 1, 0))
conf_matrix <- table(test_pred, test_dataRB$ThreeYrLeft)
conf_matrix

# Calculate metrics
precision <- conf_matrix[1, 1] / sum(conf_matrix[1, ])
recall <- conf_matrix[1, 1] / sum(conf_matrix[, 1])
f1_score <- 2 * (precision * recall) / (precision + recall)
accuracy <- mean(test_pred == test_dataRB$ThreeYrLeft)
precision
recall
f1_score
accuracy
```
Our interaction logistic model has an overall accuracy of 72%. We will use 0 (less than 3 years) as positive for the following statistics as we are focusing on retiring players.

It has a precision of 0.74, meaning of all the players the model predicted to have < 3 years left, 74% actually did. So, out of the model’s positive predictions, 74% were correct.

It has a recall of 0.89, meaning of all the players that actually had < 3 years left, the model identified 89% of them correctly. It means the model found 89% of the actual positives in the data set.

It has an F1 score of 0.81, which is a high balanced metric of recall and precision indicating this is a good model for identifying positives accurately, when a player will retire in less than 3 years.

Overall, with the addition of threshold testing, although recall decreased, overall accuracy increased for our interaction model.

## Logistic Model for WRs
```{r WR Logistic, warning = FALSE }

finalWR <- final_1[final_1$Pos == 'WR',]

train_index <- createDataPartition(finalWR$ThreeYrLeft, p = 0.8, list = FALSE)
train_dataWR <- finalWR[train_index, ]
test_dataWR <- finalWR[-train_index, ]

# Check the dimensions
dim(train_dataWR)
dim(test_dataWR)


# Ensure as.factor(train_data$FiveYrLeft)
train_dataWR$ThreeYrLeft <- as.numeric(train_dataWR$ThreeYrLeft)
test_dataWR$ThreeYrLeft <- as.numeric(test_dataWR$ThreeYrLeft)

train_dataWR<- train_dataWR %>% ungroup() %>% select(ThreeYrLeft,Age, G, GS, AllTD, Pts, TotalInjuryCount, ConcussionTotal)

test_dataWR <- test_dataWR %>% ungroup() %>% select(ThreeYrLeft, Age, G, GS, AllTD, Pts, TotalInjuryCount, ConcussionTotal)

logisticWR <- glm(ThreeYrLeft ~ ., train_dataWR, family = "binomial")
summary(logisticWR)

#AIC of 2034.7

sjPlot::plot_model(logisticWR, type ="pred")
```
```{r warning=FALSE}
predictions <- predict(logisticWR, newdata = test_dataWR, type = "response")
test_pred <- ifelse(is.na(predictions), NA, ifelse(predictions > 0.5, 1, 0))
conf_matrix <- table(test_pred, test_dataWR$ThreeYrLeft)

# Calculate metrics
precision <- conf_matrix[1, 1] / sum(conf_matrix[1, ])
recall <- conf_matrix[1, 1] / sum(conf_matrix[, 1])
f1_score <- 2 * (precision * recall) / (precision + recall)
accuracy <- mean(test_pred == test_dataWR$ThreeYrLeft)
precision
recall
f1_score
accuracy
```
Our logistic model has an overall accuracy of 61%. We will use 0 (less than 3 years) as positive for the following statistics as we are focusing on retiring players.

It has a precision of 0.63, meaning of all the players the model predicted to have < 3 years left, 63% actually did. So, out of the model’s positive predictions, 63% were correct.

It has a recall of 0.90 meaning of all the players that actually had < 3 years left, the model identified 90% of them correctly. It means the model found 90% of the actual positives in the data set.

It has an F1 score of 0.74, which is a high balanced metric of recall and precision indicating this is a good model for identifying positives accurately, when a player will retire in less than 3 years.

Of the features used in our WR logistic regression the following were significant at 5% and will be used in the mixed effects model.
Age, Games and Points had a negative correlation, meaning as they increase, the odds of playing for more than 3 years decreases.
Games Starteda and TD Scored and Points Scored had a positive correlation, meaning as they increase, the odds of playing for more than 3 years increases


## Logistic Model with Interaction for WRs
```{r WR Mixed Effects, warning=FALSE}
logisticWR1 <- glm(ThreeYrLeft ~ Age * Pts * AllTD * G * GS, train_dataWR, family = "binomial")
summary(logisticWR1)
#AIC: 1947.4
 
predictions <- predict(logisticWR1, newdata = test_dataWR, type = "response")
```
Age had a negative coefficient and the interaction between Age, Points, Touchdowns and Games Started had a marginally positive coefficient. Both were significant at 5%.

```{r warning=FALSE}

#Threshold Testing
curr = 0
for (i in seq(0.3, 0.7, by = 0.01)){
  test_pred <- ifelse(is.na(predictions), NA, ifelse(predictions > i, 1, 0))
  
  accuracy <- mean(test_pred == test_dataWR$ThreeYrLeft)
  
  
  if (accuracy > curr){
    best = c(i, accuracy)
    curr = accuracy
  }
  
}

test_pred <- ifelse(is.na(predictions), NA, ifelse(predictions > best[1], 1, 0))
conf_matrix <- table(test_pred, test_dataWR$ThreeYrLeft)

# Calculate metrics
precision <- conf_matrix[1, 1] / sum(conf_matrix[1, ])
recall <- conf_matrix[1, 1] / sum(conf_matrix[, 1])
f1_score <- 2 * (precision * recall) / (precision + recall)
accuracy <- mean(test_pred == test_dataWR$ThreeYrLeft)
precision
recall
f1_score
accuracy
```
Our interaction logistic model has an overall accuracy of 67%. We will use 0 (less than 3 years) as positive for the following statistics as we are focusing on retiring players.

It has a precision of 0.66, meaning of all the players the model predicted to have < 3 years left, 66% actually did. So, out of the model’s positive predictions, 66% were correct.

It has a recall of 0.94 meaning of all the players that actually had < 3 years left, the model identified 94% of them correctly. It means the model found 94% of the actual positives in the data set.

It has an F1 score of 0.78, which is a high balanced metric of recall and precision indicating this is a good model for identifying positives accurately, when a player will retire in less than 3 years.

## Linear Random Effects Model
```{r Random Effects, warning=FALSE}
pos_data <- final_all %>% ungroup() %>% select(Age, Pos, G, GS, AllTD, Pts, TotalInjuryCount, ConcussionTotal, YearsRemaining)




position_age <- lmer(YearsRemaining ~ Age + (1|Pos), data = pos_data)
summary(position_age)
random_effects = ranef(position_age)
ranef_df <-  data.frame(
  int = random_effects$Pos$`(Intercept)`,
  pos = rownames(random_effects$Pos)
)
performance::r2(position_age)
ggplot(ranef_df, aes(x=int, y=reorder(pos, int))) +
  geom_point() + 
  labs(x= "intercept", y="pos", title = 'Effect of Position on Years Remaining', subtitle = 'lm ~ Age') +
  theme_minimal()
```
Age has a significant negative coefficient meaning for every 0.15 years older, 1 year decrease in number of years remaining


## Linear Random Effects Model with Interaction
```{r Games, warning=FALSE}
scaled <- data.frame(cbind(scale(pos_data[,c('Age', 'G', 'YearsRemaining')]), pos_data$Pos)) %>% rename( 'Pos'='V4')
scaled$Age <- as.numeric(scaled$Age)
scaled$YearsRemaining <- as.numeric(scaled$YearsRemaining)
scaled$G <- as.numeric(scaled$G)



position_g <- lmer(YearsRemaining ~ G * Age + (1|Pos), data = scaled)
summary(position_g)
random_effects = ranef(position_g)
ranef_df <-  data.frame(
  int = random_effects$Pos$`(Intercept)`,
  pos = rownames(random_effects$Pos)
)
performance::r2(position_g)
ggplot(ranef_df, aes(x=int, y=reorder(pos, int))) +
  geom_point() + 
  labs(x= "intercept", y="pos", title = 'Effect of Position on Years Remaining', subtitle = 'lm ~ Age * G') +
  theme_minimal()
```
Age is the greatest contributor to this model as it has highest t value. From the model you can see a negative correlation between age and years remaining. As age increases by 1 year, number of years remaining decreases by 0.23 years.

Number of games has a relatively small impact on the model. It has a small positive correlation with years remaining, meaning for every game played, there is a 0.02 increase in years remaining. Depending on how you interpret the model this makes sense as if you are a better/more active players, maybe you are more likely to keep playing vs someone who doesn't get a lot of reps and retires early.

The interaction between Games and Age has a relatively small impact on the model. It has a small negative correlation meaning that for every 1 year increase in age, the effect of number of games played on years remaining decreases by 0.028.

For both random effects models, the marginal R2 was ~4% and the conditional R2 was ~20%, meaning that with the addition of different intercepts per position we could explain an extra 16% of variance in our model. This shows that position is a significant factor in retirement and each position should be treated differently in terms of career longevity.

```{r Compare to average career lengths}
average_len_careers

```
The ordering of positions in our random effects models mirror very closely the ordering of positions by average career length.


## Challenges and Limitations
One of the challenges that we had to overcome during the data wrangling part of the process was the positions being very widespread. We had to combine the wide array of positions such as Left Cornerback, Right Cornerback, and safety positions into a defensive back position group to make running the model much easier. We had to do the same for the linebacker and defensive lineman position groups. There were also a few niche positions that we elected to remove from our dataset which were kick returner, punt returner, and long snapper because all of these positions are special teams only and the vast majority of the players that played in these roles were also listed at other position groups. Another challenge we had was combining the player injuries dataset and the player stats data set by the team names. This is because the team names were listed differently.


## Conclusion and Insights
In conclusion, we had good success with predicting retirement in the next 3 years. Although our logistics models for WR and RBs had 60-70% overall accuracy, they had  particularly high recall metrics, meaning of the players that are likely to retire soon, they identified the large majority. This would be useful for coaches as if a player flags that they have less time left in the NFL, coaches could either take measures to help prolong that player's career such as minute limits, or know that they have to start searching for a replacement.

From our findings, it is obvious that Age is the most influential factor to retirement. However, depending on the positions the number of games played and touchdowns scored also proved to be significant. Whilst number of touchdowns is a stat that will always tried to be maximized in the NFL, the number of games played could be controlled by a coach as a means to increase the time left for a player in particular for RBs.

When looking at positions, we found that quarterbacks and kickers have the "highest baseline" for years remaining in the league from our linear model, which supported the statistic that they have the highest value for average career length. Of the remaining positions (not including outliers because of too few samples), defensive ends and linebackers had the "lowest baseline" for years remaining meaning they have the shortest careers. This information could be useful to coaches in order to determine the frequency by which they need to recruit new players to replace those that retired.